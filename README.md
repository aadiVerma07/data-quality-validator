# ğŸ§  Data Quality Validator

A lightweight **Python-based ETL utility** that validates raw data files (CSV/JSON) for schema consistency, nulls, duplicates, and data type mismatches before loading.  
Designed for data engineers who want a reusable pre-validation step in ETL pipelines.

---

## ğŸ“˜ Overview
In real-world ETL systems, poor-quality data can break downstream analytics.  
This project provides a configurable **Data Quality Validator** that checks data before it enters your warehouse or analytics layer.

The validator:
- Reads a file (CSV/JSON)
- Validates column schema & data types
- Detects nulls, duplicates, invalid ranges
- Generates summary reports and logs results

---

## ğŸ§± Architecture

+--------------------+
| Raw Data (CSV/JSON)| --> validation_rules.yaml
+--------------------+
|
â–¼
+---------------------+
| Data Quality Check |
| (Python + Pandas) |
+---------------------+
|
â–¼
+--------------------+
| Logs & Reports |
+--------------------+

---

## âš™ï¸ Tech Stack
| Component | Technology |
|------------|-------------|
| **Language** | Python 3 |
| **Libraries** | Pandas, PyYAML, Logging, argparse |
| **Data Sources** | CSV, JSON |
| **Output** | Console + Report file (CSV) |

---

## ğŸ§© Features
âœ… Configurable validation rules (in YAML)  
âœ… Checks for nulls, duplicates, invalid data types  
âœ… Logs invalid rows to separate files  
âœ… Generates summary report with pass/fail counts  
âœ… Modular code structure for easy reuse  

---

